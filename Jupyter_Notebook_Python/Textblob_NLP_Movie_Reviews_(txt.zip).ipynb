{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment & Similar NLP Analysis\n",
    "#### Rick Gosalvez, Spring 2020\n",
    "\n",
    "# Critic Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: imdb.com <br>\n",
    "Parasite Movie - Critic Reviews: www.imdb.com/title/tt6751668/externalreviews?ref_=tt_ov_rt <br>\n",
    "Joker Movie - Critic Reviews: www.imdb.com/title/tt7286456/externalreviews?ref_=tt_ov_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "parasite_rev = {}        # create global dictionary\n",
    "joker_rev = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Files Analyzed: 10\n",
      "txt/Joker/LA_Weekly_Negative.txt\n",
      "txt/Joker/National_Reviewer_Positive.txt\n",
      "txt/Joker/1001_Movies_Positive.txt\n",
      "txt/Joker/Reel_Views_Positive.txt\n",
      "txt/Joker/Roger_Ebert_Negative.txt\n",
      "txt/Joker/HuffPost_Negative.txt\n",
      "txt/Joker/NY_Times_Negative.txt\n",
      "txt/Joker/SF_Examiner_Positive.txt\n",
      "txt/Joker/Austin_Chronicle_Negative.txt\n",
      "txt/Joker/Sight_Sound_Positive.txt\n"
     ]
    }
   ],
   "source": [
    "# get filenames in txt subdirectory\n",
    "file = glob.glob(\"*/Joker/*.txt\", recursive=True)\n",
    "print(f'Num. Files Analyzed: {len(file)}')\n",
    "\n",
    "count = 1\n",
    "num_files = len(file)\n",
    "\n",
    "for i in file:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Popularity Score to a Comparable Rating System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct sentiment analysis on every review; use __popularity score__ to create __common rating system__ per below: <br> \n",
    "-1 to 0 Negative <br> \n",
    "0 Neutral <br>\n",
    "0 to 1 Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "review_dict = {}\n",
    "crit_dict = {}\n",
    "review = ''\n",
    "\n",
    "for i in file:\n",
    "    movie = re.search(r'/(\\w+)[^/]', i).group().strip()     # movie name for text extraction\n",
    "    critic = re.search(r'[^/](\\w+)_', i).group().strip()    # critic name for text extraction\n",
    "    critic_rating = re.search(r'_([PN][oe][gsu]\\w+)\\.', i).group().strip()   # critic name for text extraction\n",
    "    movie = movie.replace('/','')\n",
    "    critic = critic.replace('_','')\n",
    "    critic_rating = critic_rating.replace('_',' ').strip()\n",
    "    critic_rating = critic_rating.replace('.','')\n",
    "    #print(critic_rating)\n",
    "    \n",
    "    with open(i,'r') as text:\n",
    "        review = text.read()                               # get review\n",
    "        blob = TextBlob(review)                            # prep for Textblob sentiment analysis\n",
    "        pop = str(blob.sentiment)                          # get sentiment analysis and convert to string for manipulation\n",
    "        sen = re.findall('polarity=(.*),\\s', pop)          # extract popularity score\n",
    "        for iii in sen:\n",
    "            sen = float(iii)                               # convert to float\n",
    "            \n",
    "            if sen <= 0:\n",
    "                sen_assess = 'Negative'\n",
    "                pred = 0\n",
    "            elif sen >= 0:\n",
    "                sen_assess = 'Positive'\n",
    "                pred = 1\n",
    "            elif sen == 0:\n",
    "                sen_assess = 'Neutral'\n",
    "                pred = 0\n",
    "                \n",
    "            if critic_rating == 'Negative':\n",
    "                expct = 0\n",
    "            elif critic_rating == 'Positive':\n",
    "                expct = 1\n",
    "            elif critic_rating == 'Neutral':\n",
    "                expct = 0\n",
    "            \n",
    "            if sen_assess == critic_rating:\n",
    "                match = 'SUCCESS'\n",
    "                pred_success = 1\n",
    "            else:\n",
    "                match = 'FAIL'\n",
    "                pred_success = 0\n",
    "            \n",
    "            review_dict[count] = {                         # create dictionary\n",
    "                movie : {\n",
    "                    'Critic' : critic,\n",
    "                    'Pop_Score' : sen,\n",
    "                    'Sen_Rating' : sen_assess,\n",
    "                    'Critic_Rating' : critic_rating, \n",
    "                    'NLP_Success' : match,\n",
    "                    'Pred_Success' : pred_success,\n",
    "                    'Predicted' : pred,\n",
    "                    'Expected' : expct,\n",
    "                    #'Review' : review\n",
    "                }\n",
    "            }\n",
    "        if count < num_files:\n",
    "            count +=1\n",
    "        else:\n",
    "            count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joker\n",
      "['Joker']\n",
      "Critic : LAWeekly\n",
      "Pop_Score : 0.009827150818530143\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Negative\n",
      "NLP_Success : FAIL\n",
      "Pred_Success : 0\n",
      "Predicted : 1\n",
      "Expected : 0\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker']\n",
      "Critic : NationalReviewer\n",
      "Pop_Score : 0.013955233782819992\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Positive\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 1\n",
      "Expected : 1\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker']\n",
      "Critic : 1001Movies\n",
      "Pop_Score : 0.08808630952380953\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Positive\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 1\n",
      "Expected : 1\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : ReelViews\n",
      "Pop_Score : 0.0815957190957191\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Positive\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 1\n",
      "Expected : 1\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : RogerEbert\n",
      "Pop_Score : -0.04309829059829061\n",
      "Sen_Rating : Negative\n",
      "Critic_Rating : Negative\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 0\n",
      "Expected : 0\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : HuffPost\n",
      "Pop_Score : -0.010985098509850997\n",
      "Sen_Rating : Negative\n",
      "Critic_Rating : Negative\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 0\n",
      "Expected : 0\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : NYTimes\n",
      "Pop_Score : -0.03129396645021646\n",
      "Sen_Rating : Negative\n",
      "Critic_Rating : Negative\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 0\n",
      "Expected : 0\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : SFExaminer\n",
      "Pop_Score : -0.026531184864518196\n",
      "Sen_Rating : Negative\n",
      "Critic_Rating : Positive\n",
      "NLP_Success : FAIL\n",
      "Pred_Success : 0\n",
      "Predicted : 0\n",
      "Expected : 1\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : AustinChronicle\n",
      "Pop_Score : 0.029872782446311863\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Negative\n",
      "NLP_Success : FAIL\n",
      "Pred_Success : 0\n",
      "Predicted : 1\n",
      "Expected : 0\n",
      "\n",
      "Joker\n",
      "['Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker', 'Joker']\n",
      "Critic : SightSound\n",
      "Pop_Score : 0.06019782913165267\n",
      "Sen_Rating : Positive\n",
      "Critic_Rating : Positive\n",
      "NLP_Success : SUCCESS\n",
      "Pred_Success : 1\n",
      "Predicted : 1\n",
      "Expected : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_list     = []\n",
    "pred_s_list    = []\n",
    "predicted_list = []\n",
    "expct_list     = []\n",
    "\n",
    "for k,v in review_dict.items():\n",
    "    for k2, v2 in v.items():                \n",
    "        print(k2)                        # print movie name\n",
    "        movie_list.append(k2)\n",
    "        print(movie_list)\n",
    "        for k3, v3 in v2.items():\n",
    "            print(f'{k3} : {v3}')        # print nested pairs to expose individual critic reviews\n",
    "            if k3 == 'Pred_Success':\n",
    "                pred_s_list.append(v3)\n",
    "            if k3 == 'Predicted':\n",
    "                predicted_list.append(v3)\n",
    "            if k3 == 'Expected':\n",
    "                expct_list.append(v3)\n",
    "        print()\n",
    "#print(review_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_rev = len(review_dict)\n",
    "print(len(review_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(movie_list, columns=['Movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Movie  Predicted  Expected  Pred_Success\n",
      "0  Joker          1         0             0\n",
      "1  Joker          1         1             1\n",
      "2  Joker          1         1             1\n",
      "3  Joker          1         1             1\n",
      "4  Joker          0         0             1\n",
      "5  Joker          0         0             1\n",
      "6  Joker          0         0             1\n",
      "7  Joker          0         1             0\n",
      "8  Joker          1         0             0\n",
      "9  Joker          1         1             1\n"
     ]
    }
   ],
   "source": [
    "df['Predicted'] = predicted_list\n",
    "df['Expected'] = expct_list\n",
    "df['Pred_Success'] = pred_s_list\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Pred_Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Predicted  Expected  Pred_Success\n",
       "count      10.00     10.00         10.00\n",
       "mean        0.60      0.50          0.70\n",
       "std         0.52      0.53          0.48\n",
       "min         0.00      0.00          0.00\n",
       "25%         0.00      0.00          0.25\n",
       "50%         1.00      0.50          1.00\n",
       "75%         1.00      1.00          1.00\n",
       "max         1.00      1.00          1.00"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 2)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      "Movie           10 non-null object\n",
      "Predicted       10 non-null int64\n",
      "Expected        10 non-null int64\n",
      "Pred_Success    10 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 448.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movie', 'Predicted', 'Expected', 'Pred_Success']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = pd.DataFrame(movie_list, columns=['Movie'])\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two['Predicted'] = predicted_list\n",
    "df_two['Expected'] = expct_list\n",
    "df_two['Pred_Success'] = pred_s_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Pred_Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie  Predicted  Expected  Pred_Success\n",
       "0  Joker          1         0             0\n",
       "1  Joker          1         1             1\n",
       "2  Joker          1         1             1\n",
       "3  Joker          1         1             1\n",
       "4  Joker          0         0             1\n",
       "5  Joker          0         0             1\n",
       "6  Joker          0         0             1\n",
       "7  Joker          0         1             0\n",
       "8  Joker          1         0             0\n",
       "9  Joker          1         1             1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Pred_Success</th>\n",
       "      <th>Pred_Sucess1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Joker</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Joker</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movie  Predicted  Expected  Pred_Success Pred_Sucess1\n",
       "0  Joker          1         0             0          Neg\n",
       "1  Joker          1         1             1          Pos\n",
       "2  Joker          1         1             1          Pos\n",
       "3  Joker          1         1             1          Pos\n",
       "4  Joker          0         0             1          Pos\n",
       "5  Joker          0         0             1          Pos\n",
       "6  Joker          0         0             1          Pos\n",
       "7  Joker          0         1             0          Neg\n",
       "8  Joker          1         0             0          Neg\n",
       "9  Joker          1         1             1          Pos"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Neg','Pos']\n",
    "df_two['Pred_Sucess1'] = [names[i] for i in df['Pred_Success']]\n",
    "\n",
    "df_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (7, 4)\n",
      "X_test.shape  = (3, 4)\n",
      "\n",
      "y_train.shape = (7,)\n",
      "y_test.shape  = (3,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     df, df_two['Pred_Sucess1'], test_size = 0.25, random_state=0)      # by specifying a number, will always generate the same set of random numbers (important for testing!) Once solid, then can remove seed (aka: number)\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'X_test.shape  = {X_test.shape}')\n",
    "\n",
    "print()\n",
    "print(f'y_train.shape = {y_train.shape}')\n",
    "print(f'y_test.shape  = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes to numpy arrays\n",
    "y_train = y_train.to_frame()      # convert from series to numpy\n",
    "y_test  = y_test.to_frame()       # convert from series to numpy\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test  = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test  = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Pos'],\n",
       "       ['Pos'],\n",
       "       ['Pos'],\n",
       "       ['Neg'],\n",
       "       ['Pos'],\n",
       "       ['Neg'],\n",
       "       ['Pos']], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[3 2]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_true=expct_list, y_pred=predicted_list)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion, index=range(2), columns=range(2))\n",
    "axes = sns.heatmap(confusion_df, annot=True, cmap='nipy_spectral_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Language Model and Creating a spaCy Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load('en')  \n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Parasite Files Analyzed: 5\n",
      "Num. Joker Files Analyzed: 6\n",
      "['txt/Parasite/1001_Movies_Positive.txt', 'txt/Parasite/Reel_Views_Positive.txt', 'txt/Parasite/NY_Times_Positive.txt', 'txt/Parasite/Sight_Sound_Slightly_Positive.txt', 'txt/Parasite/Roger_Ebert_Positive.txt']\n",
      "['txt/Joker/Roger_Ebert_Slightly_Negative.txt', 'txt/Joker/1001_Movies_Positive.txt', 'txt/Joker/Reel_Views_Positive.txt', 'txt/Joker/NY_Times_Negative.txt', 'txt/Joker/SF_Examiner_Positive.txt', 'txt/Joker/Sight_Sound_Positive.txt']\n"
     ]
    }
   ],
   "source": [
    "# get filenames in txt and movie subdirectories\n",
    "parasite_file = glob.glob(\"*/Parasite/*.txt\", recursive=True)\n",
    "joker_file = glob.glob(\"*/Joker/*.txt\", recursive=True)\n",
    "print(f'Num. Parasite Files Analyzed: {len(parasite_file)}')\n",
    "print(f'Num. Joker Files Analyzed: {len(joker_file)}')\n",
    "\n",
    "pnum_files = len(parasite_file)\n",
    "jnum_files = len(joker_file)\n",
    "\n",
    "print(parasite_file)\n",
    "print(joker_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811083940589308"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1 = nlp(Path(parasite_file[0]).read_text())\n",
    "document2 = nlp(Path(joker_file[0]).read_text())\n",
    "document1.similarity(document2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, j=0\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/Roger_Ebert_Slightly_Negative.txt, Similarity: 0.981%\n",
      "\n",
      "i=0, j=1\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/1001_Movies_Positive.txt, Similarity: 0.990%\n",
      "\n",
      "i=0, j=2\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/Reel_Views_Positive.txt, Similarity: 0.976%\n",
      "\n",
      "i=0, j=3\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/NY_Times_Negative.txt, Similarity: 0.978%\n",
      "\n",
      "i=0, j=4\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/SF_Examiner_Positive.txt, Similarity: 0.964%\n",
      "\n",
      "i=0, j=5\n",
      "txt/Parasite/1001_Movies_Positive.txt vs txt/Joker/Sight_Sound_Positive.txt, Similarity: 0.972%\n",
      "\n",
      "i=1, j=0\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/Roger_Ebert_Slightly_Negative.txt, Similarity: 0.984%\n",
      "\n",
      "i=1, j=1\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/1001_Movies_Positive.txt, Similarity: 0.980%\n",
      "\n",
      "i=1, j=2\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/Reel_Views_Positive.txt, Similarity: 0.989%\n",
      "\n",
      "i=1, j=3\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/NY_Times_Negative.txt, Similarity: 0.989%\n",
      "\n",
      "i=1, j=4\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/SF_Examiner_Positive.txt, Similarity: 0.975%\n",
      "\n",
      "i=1, j=5\n",
      "txt/Parasite/Reel_Views_Positive.txt vs txt/Joker/Sight_Sound_Positive.txt, Similarity: 0.986%\n",
      "\n",
      "i=2, j=0\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/Roger_Ebert_Slightly_Negative.txt, Similarity: 0.986%\n",
      "\n",
      "i=2, j=1\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/1001_Movies_Positive.txt, Similarity: 0.979%\n",
      "\n",
      "i=2, j=2\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/Reel_Views_Positive.txt, Similarity: 0.987%\n",
      "\n",
      "i=2, j=3\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/NY_Times_Negative.txt, Similarity: 0.988%\n",
      "\n",
      "i=2, j=4\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/SF_Examiner_Positive.txt, Similarity: 0.980%\n",
      "\n",
      "i=2, j=5\n",
      "txt/Parasite/NY_Times_Positive.txt vs txt/Joker/Sight_Sound_Positive.txt, Similarity: 0.989%\n",
      "\n",
      "i=3, j=0\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/Roger_Ebert_Slightly_Negative.txt, Similarity: 0.985%\n",
      "\n",
      "i=3, j=1\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/1001_Movies_Positive.txt, Similarity: 0.980%\n",
      "\n",
      "i=3, j=2\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/Reel_Views_Positive.txt, Similarity: 0.985%\n",
      "\n",
      "i=3, j=3\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/NY_Times_Negative.txt, Similarity: 0.988%\n",
      "\n",
      "i=3, j=4\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/SF_Examiner_Positive.txt, Similarity: 0.981%\n",
      "\n",
      "i=3, j=5\n",
      "txt/Parasite/Sight_Sound_Slightly_Positive.txt vs txt/Joker/Sight_Sound_Positive.txt, Similarity: 0.987%\n",
      "\n",
      "i=4, j=0\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/Roger_Ebert_Slightly_Negative.txt, Similarity: 0.983%\n",
      "\n",
      "i=4, j=1\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/1001_Movies_Positive.txt, Similarity: 0.973%\n",
      "\n",
      "i=4, j=2\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/Reel_Views_Positive.txt, Similarity: 0.983%\n",
      "\n",
      "i=4, j=3\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/NY_Times_Negative.txt, Similarity: 0.984%\n",
      "\n",
      "i=4, j=4\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/SF_Examiner_Positive.txt, Similarity: 0.982%\n",
      "\n",
      "i=4, j=5\n",
      "txt/Parasite/Roger_Ebert_Positive.txt vs txt/Joker/Sight_Sound_Positive.txt, Similarity: 0.986%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,pnum_files):\n",
    "    for j in range(0,jnum_files):\n",
    "        document1 = nlp(Path(parasite_file[i]).read_text())\n",
    "        document2 = nlp(Path(joker_file[j]).read_text())\n",
    "        print(f'i={i}, j={j}')\n",
    "        print(f'{parasite_file[i]} vs {joker_file[j]}, Similarity: {document1.similarity(document2):.3f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Sentiment and Similarity analysis __need 'learning'__ to improve analysis. <br>\n",
    "<br>\n",
    "Several Joker Movie Reviews were slightly negative or negative; however, the sensetivity analysis resulted in a 'Positive' sentiment for the review, which was not accurate. <br>\n",
    "<br>\n",
    "The same was true for Similarity analysis. When comparing reviews from different reviewers and the same reviewer, the analysis estimated that all had a __high__ probability of being similar, which was not true; however, perhaps one can conclude that professional movie critics write similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth Example - Roger Ebert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Parasite Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Reviewer: Roger Ebert.\n",
       "\n",
       "Family is everything in Bong Joon-ho’s “Parasite,” a satire that pulls a series of sharp comic switcheroos for an unexpectedly poignant social comment. Bong’s “Okja” competed for the Palme for the first time in 2017, and he is well known internationally for thrillers including “Snowpiercer” and “The Host.” \n",
       "\n",
       "The unemployed Kim family of four lives in a basement apartment, where they gleefully poach on a neighbor’s Wifi and make a little cash doing piecework assembling pizza cartons for a chain. A passing nod to the triangular stink bugs that infest their cramped fetid home will take on a meaning that will eventually cast a symbolic shadow in Boon’s larger scheme of things. \n",
       "\n",
       "A posh friend of son Ki-woo, aka Kevin (Choi Woo-shik), shows up mysteriously, gifting him with a big craggy “scholar’s stone” said to bring good fortune, and passing on the connection for a gig to tutor the teen daughter of the wealthy Park family. Sister Ki-jung, aka Jessica (Park So-dam), utilizes her ace Photoshop skills to fake his university diploma. Within minutes, Kevin’s caring line of patter has twitchy and vulnerable Mrs. Park and her cute wide-eyed daughter eating out of his hand.\n",
       "\n",
       "Mrs. Park confides that she’s also looking for an art teacher for their hyperactive little son, and Kevin just happens to think of someone named Jessica—very much in demand but possibly available. Boning up on a load of art therapy jargon on the web, she also is installed as a valued expert in the Park household by the clueless lady of the house. With a combination of dirty tricks and some innocently proffered advice, the Kims soon succeed in getting dad and mom on the payroll as chauffeur and housekeeper, all four pretending to be unrelated.\n",
       "\n",
       "While the Parks are satisfied with their new employees, even as they are being massively hoodwinked, husband and wife agree on one little drawback. There’s a whiff of something unidentifiable about each of those four. Turns out it’s the dank basement smell on their clothes. “People who ride the subway have a special smell,” sniffs Mr. Park, a corporate CEO.\n",
       "\n",
       "Creating a setup in which sympathy at first builds for the filthy rich Parks, Bong takes it to the limit. Nice, trusting and gullible, they are easy marks for this family of grifters. As slickly as Bong created comedy around the Kims’ rampage of rascally shenanigans, he introduces a note of dire economic desperation with the surprise return of the former housekeeper, whom they had succeeded in ousting from her job. She has her own agenda involving a labyrinthine bunker under the post-modern mansion, and may also be out for revenge. \n",
       "\n",
       "Bit by bit, Bong succeeds in turning the tables on this plot with a conspicuous sense of righteousness, to reveal his whole crew of merry grifters as little people fighting over crumbs. A slapstick rampage turns into an imaginatively choreographed violent melee on the mansion lawn in the course of child’s extravagant birthday pageant. Just deserts are served.\")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "with open('txt/Parasite/Roger_Ebert.txt','r') as text:\n",
    "    review = text.read()\n",
    "\n",
    "blob = TextBlob(review)\n",
    "\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My read on the review was that Roger Ebert was positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Reviewer: Roger Ebert.\"),\n",
       " Sentence(\"Family is everything in Bong Joon-ho’s “Parasite,” a satire that pulls a series of sharp comic switcheroos for an unexpectedly poignant social comment.\"),\n",
       " Sentence(\"Bong’s “Okja” competed for the Palme for the first time in 2017, and he is well known internationally for thrillers including “Snowpiercer” and “The Host.” \n",
       " \n",
       " The unemployed Kim family of four lives in a basement apartment, where they gleefully poach on a neighbor’s Wifi and make a little cash doing piecework assembling pizza cartons for a chain.\"),\n",
       " Sentence(\"A passing nod to the triangular stink bugs that infest their cramped fetid home will take on a meaning that will eventually cast a symbolic shadow in Boon’s larger scheme of things.\"),\n",
       " Sentence(\"A posh friend of son Ki-woo, aka Kevin (Choi Woo-shik), shows up mysteriously, gifting him with a big craggy “scholar’s stone” said to bring good fortune, and passing on the connection for a gig to tutor the teen daughter of the wealthy Park family.\"),\n",
       " Sentence(\"Sister Ki-jung, aka Jessica (Park So-dam), utilizes her ace Photoshop skills to fake his university diploma.\"),\n",
       " Sentence(\"Within minutes, Kevin’s caring line of patter has twitchy and vulnerable Mrs. Park and her cute wide-eyed daughter eating out of his hand.\"),\n",
       " Sentence(\"Mrs. Park confides that she’s also looking for an art teacher for their hyperactive little son, and Kevin just happens to think of someone named Jessica—very much in demand but possibly available.\"),\n",
       " Sentence(\"Boning up on a load of art therapy jargon on the web, she also is installed as a valued expert in the Park household by the clueless lady of the house.\"),\n",
       " Sentence(\"With a combination of dirty tricks and some innocently proffered advice, the Kims soon succeed in getting dad and mom on the payroll as chauffeur and housekeeper, all four pretending to be unrelated.\"),\n",
       " Sentence(\"While the Parks are satisfied with their new employees, even as they are being massively hoodwinked, husband and wife agree on one little drawback.\"),\n",
       " Sentence(\"There’s a whiff of something unidentifiable about each of those four.\"),\n",
       " Sentence(\"Turns out it’s the dank basement smell on their clothes.\"),\n",
       " Sentence(\"“People who ride the subway have a special smell,” sniffs Mr. Park, a corporate CEO.\"),\n",
       " Sentence(\"Creating a setup in which sympathy at first builds for the filthy rich Parks, Bong takes it to the limit.\"),\n",
       " Sentence(\"Nice, trusting and gullible, they are easy marks for this family of grifters.\"),\n",
       " Sentence(\"As slickly as Bong created comedy around the Kims’ rampage of rascally shenanigans, he introduces a note of dire economic desperation with the surprise return of the former housekeeper, whom they had succeeded in ousting from her job.\"),\n",
       " Sentence(\"She has her own agenda involving a labyrinthine bunker under the post-modern mansion, and may also be out for revenge.\"),\n",
       " Sentence(\"Bit by bit, Bong succeeds in turning the tables on this plot with a conspicuous sense of righteousness, to reveal his whole crew of merry grifters as little people fighting over crumbs.\"),\n",
       " Sentence(\"A slapstick rampage turns into an imaginatively choreographed violent melee on the mansion lawn in the course of child’s extravagant birthday pageant.\"),\n",
       " Sentence(\"Just deserts are served.\")]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0965895892725161, subjectivity=0.5528692851863584)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.03958333333333333, subjectivity=0.45416666666666666)\n",
      "Sentiment(polarity=0.020833333333333332, subjectivity=0.27777777777777773)\n",
      "Sentiment(polarity=0.0, subjectivity=0.5)\n",
      "Sentiment(polarity=0.24, subjectivity=0.54)\n",
      "Sentiment(polarity=-0.5, subjectivity=1.0)\n",
      "Sentiment(polarity=0.0, subjectivity=0.75)\n",
      "Sentiment(polarity=0.1375, subjectivity=0.3666666666666667)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=-0.04999999999999999, subjectivity=0.75)\n",
      "Sentiment(polarity=0.11221590909090909, subjectivity=0.7386363636363636)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.17857142857142858, subjectivity=0.2857142857142857)\n",
      "Sentiment(polarity=-0.05833333333333335, subjectivity=0.6944444444444443)\n",
      "Sentiment(polarity=0.5166666666666666, subjectivity=0.9166666666666667)\n",
      "Sentiment(polarity=-0.016666666666666663, subjectivity=0.19166666666666665)\n",
      "Sentiment(polarity=0.6, subjectivity=1.0)\n",
      "Sentiment(polarity=0.23749999999999996, subjectivity=0.3333333333333333)\n",
      "Sentiment(polarity=-0.10000000000000003, subjectivity=0.85)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:   # analysis by sentence\n",
    "    print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alt Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=1.0, p_neg=2.9661571002312763e-19)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "blob = TextBlob(review, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(review, analyzer=NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=1.0, p_neg=2.9661571002312763e-19)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='neg', p_pos=0.3006329113924054, p_neg=0.6993670886075953)\n",
      "Sentiment(classification='pos', p_pos=0.9966130714798019, p_neg=0.003386928520196439)\n",
      "Sentiment(classification='pos', p_pos=0.9999251423832982, p_neg=7.485761669894754e-05)\n",
      "Sentiment(classification='pos', p_pos=0.9669015616777374, p_neg=0.03309843832226073)\n",
      "Sentiment(classification='pos', p_pos=0.9984286837804589, p_neg=0.0015713162195443943)\n",
      "Sentiment(classification='pos', p_pos=0.5038913934685341, p_neg=0.4961086065314636)\n",
      "Sentiment(classification='pos', p_pos=0.6046869548705573, p_neg=0.3953130451294407)\n",
      "Sentiment(classification='pos', p_pos=0.8936475056010803, p_neg=0.10635249439891564)\n",
      "Sentiment(classification='pos', p_pos=0.8786608286441256, p_neg=0.12133917135587752)\n",
      "Sentiment(classification='pos', p_pos=0.7248690911846424, p_neg=0.2751309088153589)\n",
      "Sentiment(classification='pos', p_pos=0.9844246724025523, p_neg=0.015575327597448304)\n",
      "Sentiment(classification='pos', p_pos=0.6922488079624052, p_neg=0.30775119203759377)\n",
      "Sentiment(classification='neg', p_pos=0.4401475490439734, p_neg=0.5598524509560263)\n",
      "Sentiment(classification='neg', p_pos=0.1518719004153452, p_neg=0.8481280995846552)\n",
      "Sentiment(classification='pos', p_pos=0.987462041483841, p_neg=0.01253795851615872)\n",
      "Sentiment(classification='pos', p_pos=0.9573285564220946, p_neg=0.04267144357790377)\n",
      "Sentiment(classification='pos', p_pos=0.990508327360274, p_neg=0.009491672639727023)\n",
      "Sentiment(classification='pos', p_pos=0.9424178095208628, p_neg=0.057582190479136236)\n",
      "Sentiment(classification='neg', p_pos=0.32024260488923373, p_neg=0.6797573951107707)\n",
      "Sentiment(classification='pos', p_pos=0.913338517809187, p_neg=0.08666148219081647)\n",
      "Sentiment(classification='pos', p_pos=0.7487743305135729, p_neg=0.25122566948642805)\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Ebert: PERSON\n",
      "Bong Joon-ho: GPE\n",
      "Parasite: ORDINAL\n",
      "Bong’s “: PERSON\n",
      "Palme: PERSON\n",
      "first: ORDINAL\n",
      "2017: DATE\n",
      "The Host: WORK_OF_ART\n",
      "Kim: PERSON\n",
      "four: CARDINAL\n",
      "’s Wifi: WORK_OF_ART\n",
      "Boon: GPE\n",
      "Ki-woo: PERSON\n",
      "Kevin: PERSON\n",
      "Choi Woo-shik: PERSON\n",
      "’s: ORG\n",
      "Park: PERSON\n",
      "Sister Ki-jung: PERSON\n",
      "Jessica: PERSON\n",
      "Park So-dam: PERSON\n",
      "minutes: TIME\n",
      "Kevin: PERSON\n",
      "Park: PERSON\n",
      "Park: PERSON\n",
      "’s: ORG\n",
      "Kevin: PERSON\n",
      "Jessica: PERSON\n",
      "Park: ORG\n",
      "Kims: ORG\n",
      "four: CARDINAL\n",
      "Parks: PERSON\n",
      "four: CARDINAL\n",
      "Park: PERSON\n",
      "first: ORDINAL\n",
      "Parks: PERSON\n",
      "Bong: PERSON\n",
      "Bong: PERSON\n",
      "Kims: ORG\n",
      "Bong: PERSON\n"
     ]
    }
   ],
   "source": [
    "for entity in document.ents:\n",
    "    print(f'{entity.text}: {entity.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parasite vs Joker Similarity Analysis for Roger Ebert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Language Model and Creating a spaCy Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the spaCy Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "document1 = nlp(Path('txt/Parasite/Roger_Ebert.txt').read_text())\n",
    "document2 = nlp(Path('txt/Joker/Roger_Ebert.txt').read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Review's Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gosalvez/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.972370561093779"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.similarity(document2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: <br>\n",
    "https://medium.com/@martinpella/naive-bayes-for-sentiment-analysis-49b37db18bf8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
